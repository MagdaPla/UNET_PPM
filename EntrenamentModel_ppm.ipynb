{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EntrenamentModel_ppm.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPz/PiP5S+i3hD+/aQYz68f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MagdaPla/UNET_PPM/blob/master/EntrenamentModel_ppm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHTjL0yDwnsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# codi preparat només per windows\n",
        "# modificat per R-colab, \n",
        "# va a llegir els directoris a GitHub https://github.com/MagdaPla/UNET_PPM.git\n",
        "# per a poder-ho llegir bé, cada vegada monto a sample data la mateixa estructura que al github\n",
        "# canvio nom de Sample_data >>> UNET_PPM\n",
        "# i pujo les dades de nou.\n",
        "\n",
        "# aquest fitxer es va desant per defecte al meu GDrive\n",
        "# cal pujar-ho també al GitHub amb les modificacions\n",
        "\n",
        "\n",
        "# comencem a preparar la informació\n",
        "\n",
        "# 1- eliminem pesos (weights) previs que hi pugui haver\n",
        "unlink(list.files(path = \"UNET_PPM/weights_r/\",full.names = TRUE))\n",
        "\n",
        "# 2- paràmetres d'entrenament\n",
        "\n",
        "epochs = 30 #epochs = 100 \n",
        "batch_size <- 10    #batch_size <- 24\n",
        "DRAW_SAMPLES = TRUE # així copia les dades de validació d'un i altre tipus al directori corresponent\n",
        "no_cores = 6   #no_cores = 12\n",
        "lr_rate = 0.0001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv3n_kr42jgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# instalem els paquets necessaris\n",
        "# a Gcolab ens cal instalarlos ne nou en cada sessió\n",
        "\n",
        "# per instal·lar rgdal al Colab ens cal incloure aquestes dues accions:\n",
        "system(\"sudo apt-get update\")\n",
        "system(\"sudo apt-get install libgdal-dev libproj-dev\")\n",
        "\n",
        "install.packages(\"rgdal\")\n",
        "install.packages(\"keras\")\n",
        "install.packages(\"tensorflow\")\n",
        "install.packages(\"reticulate\")\n",
        "install.packages(\"raster\")\n",
        "install.packages(\"abind\")\n",
        "install.packages(\"foreach\")\n",
        "install.packages(\"parallel\")\n",
        "install.packages(\"doParallel\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnXVJhSJ2coY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "367eb72c-6301-4bd3-d1e7-eb55d60c7134"
      },
      "source": [
        "library(keras)\n",
        "library(tensorflow)\n",
        "library(reticulate)\n",
        "library(raster)\n",
        "library(abind)\n",
        "library(foreach)\n",
        "library(parallel)\n",
        "library(doParallel)\n",
        "library(rgdal)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading required package: sp\n",
            "\n",
            "Loading required package: iterators\n",
            "\n",
            "rgdal: version: 1.4-8, (SVN revision 845)\n",
            " Geospatial Data Abstraction Library extensions to R successfully loaded\n",
            " Loaded GDAL runtime: GDAL 2.2.3, released 2017/11/20\n",
            " Path to GDAL shared files: /usr/share/gdal/2.2\n",
            " GDAL binary built with GEOS: TRUE \n",
            " Loaded PROJ.4 runtime: Rel. 4.9.3, 15 August 2016, [PJ_VERSION: 493]\n",
            " Path to PROJ.4 shared files: (autodetected)\n",
            " Linking to sp version: 1.4-2 \n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3MHq4pF7gw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "set.seed(104)\n",
        "\n",
        "# per a fer reproduible les probes fixem una llavor\n",
        "# cal tenir present però que el codi que tenim es basa en un exemple basat en tensorflow v1.x\n",
        "# al collab es descarrega una versió 2\n",
        "# feun una adaptació al codi\n",
        "tf<-tf$compat.v1\n",
        "tf$set_random_seed(100)\n",
        "\n",
        "# si no funcionés, canviem el valor \"1\"\n",
        "gpu_options <- tf$GPUOptions(allow_growth=TRUE, per_process_gpu_memory_fraction = 1) #tf$GPUOptions(per_process_gpu_memory_fraction = 0.3)\n",
        "config <- tf$ConfigProto(gpu_options = gpu_options)\n",
        "\n",
        "session_conf <- config\n",
        "sess <- tf$Session(graph = tf$get_default_graph(), config = session_conf)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h65SxFmB8Fi-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Paràmetres -----------------------------------------------------\n",
        "\n",
        "# directoris de les imatges i màscares preparades per l'entrenament/validació\n",
        "images_dir <- \"UNET_PPM/rgb1/\" \n",
        "masks_dir <- \"UNET_PPM/masc1/\"\n",
        "\n",
        "# \n",
        "if (DRAW_SAMPLES) {\n",
        "  \n",
        "  unlink(list.files(path = \"UNET_PPM/valid_masc/\",full.names = TRUE))\n",
        "  unlink(list.files(path = \"UNET_PPM/valid_rgb/\",full.names = TRUE))\n",
        "  \n",
        "  # nombre d'imatges per l'entrenament (80%)\n",
        "  train_samples <- length(list.files(images_dir)) #144\n",
        "  train_index <- sample(1:train_samples, round(train_samples * 0.8))\n",
        "  val_index <- c(1:train_samples)[-train_index]\n",
        "  \n",
        "  \n",
        "  # desar les imatges de validació a:\n",
        "  valid_save=list.files(images_dir,full.names = TRUE)\n",
        "  valid_save=valid_save[val_index]\n",
        "  file.copy(from=valid_save,to=\"UNET_PPM/valid_rgb/\")\n",
        "  \n",
        "  valid_save=list.files(masks_dir,full.names = TRUE)\n",
        "  valid_save=valid_save[val_index]\n",
        "  file.copy(from=valid_save,to=\"UNET_PPM/valid_masc/\")\n",
        "  \n",
        "  save(train_index, val_index, file = \"UNET_PPM/train_val_indices.RData\")\n",
        "  \n",
        "} else {\n",
        "  load(\"UNET_PPM/train_val_indices.RData\", verbose=T)\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcPoh92I8qTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Funció \"Loss\"  -----------------------------------------------------\n",
        "\n",
        "# hem d'adaptar la manera de fer el set_session de keras v2 a v1\n",
        "K<-tf$compat.v1.keras.backend\n",
        "K$set_session(sess)\n",
        "\n",
        "dice_coef <- custom_metric(\"custom\", function(y_true, y_pred, smooth = 1.0) {\n",
        "  y_true_f <- k_flatten(y_true)\n",
        "  y_pred_f <- k_flatten(y_pred)\n",
        "  intersection <- k_sum(y_true_f * y_pred_f)\n",
        "  result <- (2 * intersection + smooth) / \n",
        "    (k_sum(y_true_f) + k_sum(y_pred_f) + smooth)\n",
        "  return(result)\n",
        "})\n",
        "\n",
        "bce_dice_loss <- function(y_true, y_pred) {\n",
        "  result <- loss_binary_crossentropy(y_true, y_pred) +\n",
        "    (1 - dice_coef(y_true, y_pred))\n",
        "  return(result)\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcN6dhhdADb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# afegim \"sequential()\" abans de córrer al model (no inclos en el codi original)\n",
        "model <- keras_model_sequential()\n",
        "\n",
        "# U-net 128 -----------------------------------------------------\n",
        "\n",
        "get_unet_128 <- function(input_shape = c(128, 128, 3),\n",
        "                         num_classes = 1) {\n",
        "  \n",
        "  inputs <- layer_input(shape = input_shape)\n",
        "  # 128\n",
        "  \n",
        "  down1 <- inputs %>%\n",
        "    layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") %>%\n",
        "    layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") \n",
        "  down1_pool <- down1 %>%\n",
        "    layer_max_pooling_2d(pool_size = c(2, 2), strides = c(2, 2))\n",
        "  # 64\n",
        "  \n",
        "  down2 <- down1_pool %>%\n",
        "    layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") %>%\n",
        "    layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") \n",
        "  down2_pool <- down2 %>%\n",
        "    layer_max_pooling_2d(pool_size = c(2, 2), strides = c(2, 2))\n",
        "  # 32\n",
        "  \n",
        "  down3 <- down2_pool %>%\n",
        "    layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") %>%\n",
        "    layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") \n",
        "  down3_pool <- down3 %>%\n",
        "    layer_max_pooling_2d(pool_size = c(2, 2), strides = c(2, 2))\n",
        "  # 16\n",
        "  \n",
        "  down4 <- down3_pool %>%\n",
        "    layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") %>%\n",
        "    layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") \n",
        "  down4_pool <- down4 %>%\n",
        "    layer_max_pooling_2d(pool_size = c(2, 2), strides = c(2, 2))\n",
        "  # 8\n",
        "  \n",
        "  center <- down4_pool %>%\n",
        "    layer_conv_2d(filters = 1024, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") %>%\n",
        "    layer_conv_2d(filters = 1024, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") \n",
        "  # center\n",
        "  \n",
        "  up4 <- center %>%\n",
        "    layer_upsampling_2d(size = c(2, 2)) %>%\n",
        "    {layer_concatenate(inputs = list(down4, .), axis = 3)} %>%\n",
        "    layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") %>%\n",
        "    layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") %>%\n",
        "    layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\")\n",
        "  # 16\n",
        "  \n",
        "  up3 <- up4 %>%\n",
        "    layer_upsampling_2d(size = c(2, 2)) %>%\n",
        "    {layer_concatenate(inputs = list(down3, .), axis = 3)} %>%\n",
        "    layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") %>%\n",
        "    layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") %>%\n",
        "    layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\")\n",
        "  # 32\n",
        "  \n",
        "  up2 <- up3 %>%\n",
        "    layer_upsampling_2d(size = c(2, 2)) %>%\n",
        "    {layer_concatenate(inputs = list(down2, .), axis = 3)} %>%\n",
        "    layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") %>%\n",
        "    layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") %>%\n",
        "    layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\")\n",
        "  # 64\n",
        "  \n",
        "  up1 <- up2 %>%\n",
        "    layer_upsampling_2d(size = c(2, 2)) %>%\n",
        "    {layer_concatenate(inputs = list(down1, .), axis = 3)} %>%\n",
        "    layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") %>%\n",
        "    layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") %>%\n",
        "    layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\")\n",
        "  # 128\n",
        "  \n",
        "  classify <- layer_conv_2d(up1,\n",
        "                            filters = num_classes, \n",
        "                            kernel_size = c(1, 1),\n",
        "                            activation = \"softmax\") #sigmoid\n",
        "  \n",
        "  \n",
        "  model <- keras_model(\n",
        "    inputs = inputs,\n",
        "    outputs = classify\n",
        "  )\n",
        "  \n",
        "  model %>% compile(\n",
        "    optimizer = optimizer_rmsprop(), #optimizer_rmsprop(lr = 0.0001)\n",
        "    loss = bce_dice_loss,\n",
        "    metrics =  c(dice_coef) \n",
        "  )\n",
        "  \n",
        "  return(model)\n",
        "}\n",
        "\n",
        "model <- get_unet_128()\n",
        "model\n",
        "\n",
        "# per utilitzar \"pesos\" d'entrenaments previes utilitzar la funció: \"load_model_weights_hdf5\"\n",
        "# per exemple:\n",
        "# load_model_weights_hdf5(model, \"./weights_r_save/unet64_178.h5\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzOkOrzKAbZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## definim el nombre de clusters\n",
        "cl <- makePSOCKcluster(no_cores) \n",
        "\n",
        "clusterEvalQ(cl, {\n",
        "  \n",
        "  library(abind)     \n",
        "  library(raster)\n",
        "  library(reticulate)\n",
        "  \n",
        "  # Llegim funcions d'augment -----------------------------------------------------\n",
        "  \n",
        "  imagesRead <- function(image_file,mask_file)\n",
        "  {\n",
        "    \n",
        "    img <- brick(image_file)\n",
        "    mask <- raster(mask_file)\n",
        "    \n",
        "    return(list(img = img, mask = mask))\n",
        "  }\n",
        "  \n",
        "  # randomHorizontalFlip : rotacions i inversions + rotacions\n",
        "  randomHorizontalFlip <- function(img,mask,u = 0) {\n",
        "    if (rnorm(1) < u) return(list(img = img, mask = mask))\n",
        "    r_angle=sample(c(2,3,4,5,6,7,8),1)\n",
        "    if(r_angle==2) {return(list(img = flip(t(img),direction = 1), mask = flip(t(mask),direction = 1)))}\n",
        "    if(r_angle==3) {return(list(img = flip(t(flip(t(img),direction = 1)),direction = 1), mask = flip(t(flip(t(mask),direction = 1)),direction = 1)))}\n",
        "    if(r_angle==4) {return(list(img = flip(t(img),direction = 2), mask = flip(t(mask),direction = 2)))}\n",
        "    if(r_angle==5) {return(list(img = flip(img,direction = 1), mask = flip(mask,direction = 1)))}\n",
        "    if(r_angle==6) {return(list(img = flip(t(flip(img,direction = 1)),direction = 1), mask = flip(t(flip(mask,direction = 1)),direction = 1)))}\n",
        "    if(r_angle==7) {return(list(img = flip(t(flip(t(flip(img,direction = 1)),direction = 1)),direction = 1), mask = flip(t(flip(t(flip(mask,direction = 1)),direction = 1)),direction = 1)))}\n",
        "    if(r_angle==8) {return(list(img = flip(t(flip(img,direction = 1)),direction = 2), mask = flip(t(flip(mask,direction = 1)),direction = 2)))}\n",
        "  }\n",
        "  \n",
        "  # add a shift to the bands\n",
        "  randomVariability = function(img, u = 0, variability = c(90, 110)) {\n",
        "    if (rnorm(1) < u) return(img)\n",
        "    variability_shift = runif(1, variability[1], variability[2])/100\n",
        "    img = img * variability_shift\n",
        "    return(img)\n",
        "  }\n",
        "  \n",
        "  \n",
        "  img2arr <- function(image) {\n",
        "    image <- as.array(image)\n",
        "    result <- aperm(image, c(2,1,3))\n",
        "    result <- result/255 # to have values between 0 and 1\n",
        "    array_reshape(result,  c(1, dim(image)[1], dim(image)[2], dim(image)[3]))\n",
        "  }\n",
        "  \n",
        "  \n",
        "  mask2arr <- function(mask) {\n",
        "    mask=as.array(mask[[1]])\n",
        "    result <- aperm(mask, c(2,1,3))\n",
        "    result=result[,,1]\n",
        "    array_reshape(result,  c(1, dim(mask)[1], dim(mask)[2], dim(mask)[3]))\n",
        "  }\n",
        "  \n",
        "})\n",
        "\n",
        "\n",
        "registerDoParallel(cl)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3ZGInTRBH46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_generator <- function(images_dir, \n",
        "                            samples_index,\n",
        "                            masks_dir, \n",
        "                            batch_size) {\n",
        "  images_iter <- list.files(images_dir, \n",
        "                            pattern = \".tif\", \n",
        "                            full.names = TRUE)[samples_index] # for current epoch\n",
        "  images_all <- list.files(images_dir, \n",
        "                           pattern = \".tif\",\n",
        "                           full.names = TRUE)[samples_index]  # for next epoch\n",
        "  masks_iter <- list.files(masks_dir, \n",
        "                           pattern = \".tif\",\n",
        "                           full.names = TRUE)[samples_index] # for current epoch\n",
        "  masks_all <- list.files(masks_dir, \n",
        "                          pattern = \".tif\",\n",
        "                          full.names = TRUE)[samples_index] # for next epoch\n",
        "  \n",
        "  function() {\n",
        "      \n",
        "    # start new epoch\n",
        "    if (length(images_iter) < batch_size) {\n",
        "      images_iter <<- images_all\n",
        "      masks_iter <<- masks_all\n",
        "    }\n",
        "    \n",
        "    batch_ind <- sample(1:length(images_iter), batch_size)\n",
        "    \n",
        "    batch_images_list <- images_iter[batch_ind]\n",
        "    images_iter <<- images_iter[-batch_ind]\n",
        "    batch_masks_list <- masks_iter[batch_ind]\n",
        "    masks_iter <<- masks_iter[-batch_ind]\n",
        "    \n",
        "    \n",
        "    x_y_batch <- foreach(i = 1:batch_size) %dopar% {\n",
        "      x_y_imgs <- imagesRead(image_file = batch_images_list[i],\n",
        "                             mask_file = batch_masks_list[i])\n",
        "      \n",
        "      # flip all side and invert\n",
        "      x_y_imgs <- randomHorizontalFlip(x_y_imgs$img,x_y_imgs$mask)\n",
        "      \n",
        "      # add some variability to the values\n",
        "      x_y_imgs$img = randomVariability(x_y_imgs$img, u = 0, variability = c(90, 110))\n",
        "      \n",
        "      # return as arrays\n",
        "      x_y_arr <- list(x = img2arr(x_y_imgs$img),\n",
        "                      y = mask2arr(x_y_imgs$mask))\n",
        "    }\n",
        "    \n",
        "    x_y_batch <- purrr::transpose(x_y_batch)\n",
        "    \n",
        "    x_batch <- do.call(abind, c(x_y_batch$x, list(along = 1)))\n",
        "    \n",
        "    y_batch <- do.call(abind, c(x_y_batch$y, list(along = 1)))\n",
        "    \n",
        "    result <- list(keras_array(x_batch), keras_array(y_batch))\n",
        "    return(result)\n",
        "  }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWgIVMo3BRlr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_generator <- function(images_dir, \n",
        "                          samples_index,\n",
        "                          masks_dir, \n",
        "                          batch_size) {\n",
        "  images_iter <- list.files(images_dir, \n",
        "                            pattern = \".tif\", \n",
        "                            full.names = TRUE)[samples_index] # for current epoch\n",
        "  images_all <- list.files(images_dir, \n",
        "                           pattern = \".tif\",\n",
        "                           full.names = TRUE)[samples_index]  # for next epoch\n",
        "  masks_iter <- list.files(masks_dir, \n",
        "                           pattern = \".tif\",\n",
        "                           full.names = TRUE)[samples_index] # for current epoch\n",
        "  masks_all <- list.files(masks_dir, \n",
        "                          pattern = \".tif\",\n",
        "                          full.names = TRUE)[samples_index] # for next epoch\n",
        "  \n",
        "  function() {\n",
        "    \n",
        "    # start new epoch\n",
        "    if (length(images_iter) < batch_size) {\n",
        "      images_iter <<- images_all\n",
        "      masks_iter <<- masks_all\n",
        "    }\n",
        "    \n",
        "    batch_ind <- sample(1:length(images_iter), batch_size)\n",
        "    \n",
        "    batch_images_list <- images_iter[batch_ind]\n",
        "    images_iter <<- images_iter[-batch_ind]\n",
        "    batch_masks_list <- masks_iter[batch_ind]\n",
        "    masks_iter <<- masks_iter[-batch_ind]\n",
        "    \n",
        "    \n",
        "    x_y_batch <- foreach(i = 1:batch_size) %dopar% {\n",
        "      x_y_imgs <- imagesRead(image_file = batch_images_list[i],\n",
        "                             mask_file = batch_masks_list[i])\n",
        "      # without augmentation\n",
        "      ########################################\n",
        "      ########################################\n",
        "      # return as arrays\n",
        "      x_y_arr <- list(x = img2arr(x_y_imgs$img),\n",
        "                      y = mask2arr(x_y_imgs$mask))\n",
        "    }\n",
        "    \n",
        "    x_y_batch <- purrr::transpose(x_y_batch)\n",
        "    \n",
        "    x_batch <- do.call(abind, c(x_y_batch$x, list(along = 1)))\n",
        "    \n",
        "    y_batch <- do.call(abind, c(x_y_batch$y, list(along = 1)))\n",
        "    \n",
        "    result <- list(keras_array(x_batch), keras_array(y_batch))\n",
        "    return(result)\n",
        "  }\n",
        "}\n",
        "\n",
        "train_iterator <- py_iterator(train_generator(images_dir = images_dir,\n",
        "                                              masks_dir = masks_dir,\n",
        "                                              samples_index = train_index,\n",
        "                                              batch_size = batch_size))\n",
        "\n",
        "val_iterator <- py_iterator(val_generator(images_dir = images_dir,\n",
        "                                          masks_dir = masks_dir,\n",
        "                                          samples_index = val_index,\n",
        "                                          batch_size = batch_size))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm6ShPVWBbcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Entrenament -----------------------------------------------------\n",
        "\n",
        "# callbacks\n",
        "callbacks_list <- list(\n",
        "  callback_model_checkpoint(filepath = \"UNET_PPM/weights_r/unet64_{epoch:03d}.h5\",\n",
        "                            monitor = \"val_custom\",\n",
        "                            save_best_only = FALSE,\n",
        "                            save_weights_only = TRUE,\n",
        "                            mode = \"max\" ,save_freq = TRUE)\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9pYk1onBuGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model %>% fit_generator(\n",
        "  generator=train_iterator,\n",
        "  steps_per_epoch = as.integer(length(train_index) / batch_size),\n",
        "  epochs = epochs,\n",
        "  validation_data = val_iterator,\n",
        "  validation_steps = as.integer(length(val_index) / batch_size),\n",
        "  verbose = 1,  callbacks = callbacks_list\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}