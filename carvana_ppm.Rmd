---
title: "Carvana Image Masking Challenge"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Carvana Image Masking Challenge}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

```{r setup}
knitr::opts_chunk$set(warning = TRUE, message = TRUE)

library(unet)
library(keras)
library(tfdatasets)
library(tidyverse)
library(rsample)
library(reticulate)

install.packages("tibble")
library(tibble)
library(magick)
```

In this example we will use the `unet` package to create a U-Net model that
could be used to remove the background from images in the Carvana dataset.

[U-Net](https://arxiv.org/abs/1505.04597) is a kind of convolutional neural network that was first developed for biomedical image segmentation but it showed good results in many other fields. 

![U-Net architecture](https://user-images.githubusercontent.com/4706822/63275620-3c987800-c278-11e9-9d92-66d1264eb05c.png)



The dataset we are going to use appeared first in the [Carvana Image Masking Challenge](https://www.kaggle.com/c/carvana-image-masking-challenge/overview)
on Kaggle. You can see more information on the [competition page](https://www.kaggle.com/c/carvana-image-masking-challenge/overview).

Before running the script, download [the data](https://www.kaggle.com/c/carvana-image-masking-challenge/data). You will only
need `train.zip` and `train_mask.zip` files.

The `train.zip` file contains images of the cars taken by Carvana and `train_mask.zip`
contains the respective masks. 

Here are some examples of what you can find in the dataset. On the left we can find the
original image and on the right we find the mask.

```{r echo=FALSE}

setwd("C:/Taller/UOC/Aules/TFM/Exemples/UNET_PPM")
library(here)

images <- tibble(
  img = list.files(here::here("rgb_jpg"), full.names = TRUE),
  mask = list.files(here::here("masc_gif"), full.names = TRUE)
  ) #%>% 
#sample_n(2) %>% 
#map(. %>% magick::image_read() %>% magick::image_resize("128x128"))

out <- magick::image_append(c(
  magick::image_append(images$img, stack = TRUE), 
  magick::image_append(images$mask, stack = TRUE)
  )
)


plot(out)
```

Now let's start building building our model. We will use `tfdatasets` to build our
data loading and pre-processing pipeline. 

First we will define which images we are going to use for training and which images
we will use for validation. I am assuming we extracted both folders into the `data-raw` directory.

```{r}
data <- tibble(
  img = list.files(here::here("rgb_jpg"), full.names = TRUE),
  mask = list.files(here::here("masc_gif"), full.names = TRUE)
)

data <- initial_split(data, prop = 0.8)

```

Ok, now let's define a pipeline to read the files and decode them as images. In
this case the images are `.jpeg` files and the masks are `.gif` files.

```{r}
training_dataset <- training(data) %>%  
  tensor_slices_dataset()%>% 
  dataset_map(~.x %>% list_modify(
    img = tf$image$decode_jpeg(tf$io$read_file(.x$img)),
    mask = tf$image$decode_gif(tf$io$read_file(.x$mask))[1,,,][,,1,drop=FALSE]
  ))
```

The `[` calls wouldn't be necessary if `tf$image$decode_gif` returned a 3D Tensor like `tf$image$decode_jpeg` does. And if it could read just one color channel as we are only interested if it's black and white.

If you are running this code interactively you can easily see the output of this
chunk with:

```{r}
example <- training_dataset %>% as_iterator() %>% iter_next()
```

The above loaded the images into into a `uint8` Tensor. Which is great for reading 
as it uses less memory. However for modelling we prefer having `float32` Tensors, 
and that the values are in the [0,1] range. That's what we will fix now:

```{r}
training_dataset <- training_dataset %>% 
  dataset_map(~.x %>% list_modify(
    img = tf$image$convert_image_dtype(.x$img, dtype = tf$float32),
    mask = tf$image$convert_image_dtype(.x$mask, dtype = tf$float32)
  ))
```

The images from our dataset are pretty high definition (1280x1918) but we will resize them to reduce the computing cost of the model. We are going to resize them to 128x128. This size is completely arbitrary.

```{r}
#això no ho aplico pq ja les tinc a 128x128
#training_dataset <- training_dataset %>% 
#  dataset_map(~.x %>% list_modify(
#    img = tf$image$resize(.x$img, size = shape(128, 128)),
#    mask = tf$image$resize(.x$mask, size = shape(128, 128))
#  ))
```

We can plot the resulting images:

```{r}
example <- training_dataset %>% as_iterator() %>% iter_next()
example$img %>% as.array() %>% as.raster() %>% plot()
```

It's usual when fitting U-Net to use some kind of data augmentation strategy.
In this example we are going to apply some random brightness, saturation and
contrast in each image. Let's encapsulate this into an R function:

```{r}
random_bsh <- function(img) {
  img %>% 
    tf$image$random_brightness(max_delta = 0.3) %>% 
    tf$image$random_contrast(lower = 0.5, upper = 0.7) %>% 
    tf$image$random_saturation(lower = 0.5, upper = 0.7) %>% 
    tf$clip_by_value(0, 1) # clip the values into [0,1] range.
}
```

We can now map this function over the images:

```{r}
training_dataset <- training_dataset %>% 
  dataset_map(~.x %>% list_modify(
    img = random_bsh(.x$img)
  ))
```

Again, we can plot the resulting image:

```{r}
example <- training_dataset %>% as_iterator() %>% iter_next()
example$img %>% as.array() %>% as.raster() %>% plot()
```

Of course, we could create a function with the above code and reuse it to create
the validation dataset, and that's what we are going to do.

```{r}
create_dataset <- function(data, train, batch_size = 32L) {
  
  dataset <- data %>% 
    tensor_slices_dataset() %>% 
    dataset_map(~.x %>% list_modify(
      img = tf$image$decode_jpeg(tf$io$read_file(.x$img)),
      mask = tf$image$decode_gif(tf$io$read_file(.x$mask))[1,,,][,,1,drop=FALSE]
    )) %>% 
    dataset_map(~.x %>% list_modify(
      img = tf$image$convert_image_dtype(.x$img, dtype = tf$float32),
      mask = tf$image$convert_image_dtype(.x$mask, dtype = tf$float32)
    ))
  
  if (train) {
    dataset <- dataset %>% 
      dataset_map(~.x %>% list_modify(
        img = random_bsh(.x$img)
      )) 
  }
  
  if (train) {
    dataset <- dataset %>% 
      dataset_shuffle(buffer_size = batch_size*128)
  }
  
  dataset <- dataset %>% 
    dataset_batch(batch_size)
  
  
  
  dataset %>% 
    dataset_map(unname) # Keras needs an unnamed output.
}
```


Note that we added 3 steps in the `create_dataset` function:

1. `dataset_batch` to batch the dataset before.
2. `dataset_shuffle` to shuffle the dataset
3. `dataset_map(unname)` since Keras needs unnamed input.

Now we can create our training and validation datasets:

```{r}
training_dataset <- create_dataset(training(data), train = TRUE)
validation_dataset <- create_dataset(testing(data), train = FALSE)

```

Great! We have prepared our data pipeline. Now we need to build the model.

Luckily, building the model is the easiest part if you use `unet`.

```{r}
#model <- unet(input_shape = c(128, 128, 3))
```

si fem el model adaptat de Unet-imatges satèl·lit-boscos
```{r}
# per calcular el paràmetre "metric" de manera personalitzada: 
# mirar però si em convé més utilitzar "binari crosentropy" o aquest... valoro
dice_coef <- custom_metric("custom", function(y_true, y_pred, smooth = 1.0) {
  y_true_f <- k_flatten(y_true)
  y_pred_f <- k_flatten(y_pred)
  intersection <- k_sum(y_true_f * y_pred_f)
  result <- (2 * intersection + smooth) / 
    (k_sum(y_true_f) + k_sum(y_pred_f) + smooth)
  return(result)
})

# per calcular el paràmetre "loss"
bce_dice_loss <- function(y_true, y_pred) {
  result <- loss_binary_crossentropy(y_true, y_pred) +
    (1 - dice_coef(y_true, y_pred))
  return(result)
}


#dice <- custom_metric("dice", function(y_true, y_pred, smooth = 1.0) {
#  y_true_f <- k_flatten(y_true)
#  y_pred_f <- k_flatten(y_pred)
#  intersection <- k_sum(y_true_f * y_pred_f)
#  (2 * intersection + smooth) / (k_sum(y_true_f) + k_sum(y_pred_f) + smooth)
#})

```


```{r}
model <- keras_model_sequential()

get_unet_128 <- function(input_shape = c(128, 128, 3),
                         num_classes = 1) {
  
  inputs <- layer_input(shape = input_shape)
  # 128
  
  down1 <- inputs %>%
    layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = "same") %>%
    layer_batch_normalization() %>%
    layer_activation("relu") %>%
    layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = "same") %>%
    layer_batch_normalization() %>%
    layer_activation("relu") 
  down1_pool <- down1 %>%
    layer_max_pooling_2d(pool_size = c(2, 2), strides = c(2, 2))
  # 64
  
  down2 <- down1_pool %>%
    layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = "same") %>%
    layer_batch_normalization() %>%
    layer_activation("relu") %>%
    layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = "same") %>%
    layer_batch_normalization() %>%
    layer_activation("relu") 
  down2_pool <- down2 %>%
    layer_max_pooling_2d(pool_size = c(2, 2), strides = c(2, 2))
  # 32
  
  down3 <- down2_pool %>%
    layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = "same") %>%
    layer_batch_normalization() %>%
    layer_activation("relu") %>%
    layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = "same") %>%
    layer_batch_normalization() %>%
    layer_activation("relu") 
  down3_pool <- down3 %>%
    layer_max_pooling_2d(pool_size = c(2, 2), strides = c(2, 2))
  # 16
  
  down4 <- down3_pool %>%
    layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = "same") %>%
    layer_batch_normalization() %>%
    layer_activation("relu") %>%
    layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = "same") %>%
    layer_batch_normalization() %>%
    layer_activation("relu") 
  down4_pool <- down4 %>%
    layer_max_pooling_2d(pool_size = c(2, 2), strides = c(2, 2))
  # 8
  
  center <- down4_pool %>%
    layer_conv_2d(filters = 1024, kernel_size = c(3, 3), padding = "same") %>%
    layer_batch_normalization() %>%
    layer_activation("relu") %>%
    layer_conv_2d(filters = 1024, kernel_size = c(3, 3), padding = "same") %>%
    layer_batch_normalization() %>%
    layer_activation("relu") 
  # center
  
  up4 <- center %>%
    layer_upsampling_2d(size = c(2, 2)) %>%
    {layer_concatenate(inputs = list(down4, .), axis = 3)} %>%
    layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = "same") %>%
    layer_batch_normalization() %>%
    layer_activation("relu") %>%
    layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = "same") %>%
    layer_batch_normalization() %>%
    layer_activation("relu") %>%
    layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = "same") %>%
    layer_batch_normalization() %>%
    layer_activation("relu")
  # 16
  
  up3 <- up4 %>%
    layer_upsampling_2d(size = c(2, 2)) %>%
    {layer_concatenate(inputs = list(down3, .), axis = 3)} %>%
    layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = "same") %>%
    layer_batch_normalization() %>%
    layer_activation("relu") %>%
    layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = "same") %>%
    layer_batch_normalization() %>%
    layer_activation("relu") %>%
    layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = "same") %>%
    layer_batch_normalization() %>%
    layer_activation("relu")
  # 32
  
  up2 <- up3 %>%
    layer_upsampling_2d(size = c(2, 2)) %>%
    {layer_concatenate(inputs = list(down2, .), axis = 3)} %>%
    layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = "same") %>%
    layer_batch_normalization() %>%
    layer_activation("relu") %>%
    layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = "same") %>%
    layer_batch_normalization() %>%
    layer_activation("relu") %>%
    layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = "same") %>%
    layer_batch_normalization() %>%
    layer_activation("relu")
  # 64
  
  up1 <- up2 %>%
    layer_upsampling_2d(size = c(2, 2)) %>%
    {layer_concatenate(inputs = list(down1, .), axis = 3)} %>%
    layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = "same") %>%
    layer_batch_normalization() %>%
    layer_activation("relu") %>%
    layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = "same") %>%
    layer_batch_normalization() %>%
    layer_activation("relu") %>%
    layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = "same") %>%
    layer_batch_normalization() %>%
    layer_activation("relu")
  # 128
  
  classify <- layer_conv_2d(up1,
                            filters = num_classes, 
                            kernel_size = c(1, 1),
                            activation = "sigmoid")
  
  
  model <- keras_model(
    inputs = inputs,
    outputs = classify
  )
  
  model %>% compile(
    optimizer = optimizer_rmsprop(lr = 0.0001),
    loss = bce_dice_loss,
    metrics = c(dice_coef)
  )
  
  return(model)
}

model <- get_unet_128()

```
That's all. The model is built. You can see the summary if you want with:
```{r}
summary(model)
```

We can now compile our model:

```{r}
#model %>% compile(
#  optimizer = optimizer_rmsprop(lr = 1e-5),
#  loss = "binary_crossentropy",
#  metrics = list(dice, metric_binary_accuracy)
#)
```

We could use a different loss - tuned to make Dice higher, but let's just use the 
binary crossentropy.

```{r}
model %>% fit(
  training_dataset,
  epochs = 3, 
  validation_data = validation_dataset
)
```

Fitting this model takes ~1500s per epoch on my MacBook Pro CPU. With a good GPU
you can make it in around ~120s/epoch.

That's it. Now you have trained a U-Net using `unet`.

We can now make predictions for the validation data and see what the results looks
like. Let's take the first batch of images in the validation data.

```{r}
batch <- validation_dataset %>% as_iterator() %>% iter_next()
predictions <- predict(model, batch)
```

In the image below you can see the original mask, the original picture and
the predicted mask. 

```{r, echo = FALSE}
images <- tibble(
  image = batch[[1]] %>% array_branch(1),
  predicted_mask = predictions[,,,1] %>% array_branch(1),
  mask = batch[[2]][,,,1]  %>% array_branch(1)
) %>% 
  sample_n(2) %>% 
  map_depth(2, function(x) {
    as.raster(x) %>% magick::image_read()
  }) %>% 
  map(~do.call(c, .x))


out <- magick::image_append(c(
  magick::image_append(images$mask, stack = TRUE),
  magick::image_append(images$image, stack = TRUE), 
  magick::image_append(images$predicted_mask, stack = TRUE)
  )
)

plot(out)
```







